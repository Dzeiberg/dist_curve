{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms\n",
    "\n",
    "> Functions to apply class-prior-preserving univariate transforms to data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def prepFeatures(X):\n",
    "    \"Apply z-score normalization to nxd feature matrix\"\n",
    "    ss = StandardScaler(with_mean=True, with_std=True)\n",
    "    Xz = ss.fit_transform(X)\n",
    "    return Xz,ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def trainOOBClassifier(X,y, modelFactory=lambda: DecisionTreeClassifier(),n_estimators=100):\n",
    "    \"\"\"\n",
    "    Train ensemble of <n_estimators> models predicting the probability that each\n",
    "    instance came from the labeled positive, rather than the unlabeled mixture, set.\n",
    "    \n",
    "    Required Arguments:\n",
    "        - X : ndarray shape (n,d) : feature matrix\n",
    "        - y : ndarray shape (n,)  : positive v. unlabeled component assignments for each instance\n",
    "    Optional Arguments:\n",
    "        - modelFactory : lambda function returning sklearn-style model instance (has fit, fit_predict, predict_proba, ... functions) : default DicisionTreeRegressor\n",
    "        - n_estimators : size of the ensemble : default 100\n",
    "        \n",
    "    Returns\n",
    "        - transform_scores : ndarray (n,) : probability that each instance came from labeled positive set, calculating using out-of-bag scores\n",
    "        - auc_pu : float : the AUROC of this non-traditional classifier\n",
    "    \"\"\"\n",
    "    # z-score normalization is applied to the whole dataset prior to training\n",
    "    X,ss = prepFeatures(X)\n",
    "    clf = BaggingClassifier(n_jobs=-1,base_estimator=modelFactory(), n_estimators=n_estimators,\n",
    "                            max_samples=X.shape[0],max_features=X.shape[1], bootstrap=True,\n",
    "                            bootstrap_features=False, oob_score=True).fit(X,y)\n",
    "    transform_scores = clf.oob_decision_function_[:,1]\n",
    "    auc_pu = roc_auc_score(y, transform_scores)\n",
    "    return transform_scores, auc_pu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def trainKFoldClassifier(X,y, modelFactory=lambda: SVC(probability=True, degree=1),KFoldValue=10):\n",
    "    \"\"\"\n",
    "    Train model using K-fold cross-validation\n",
    "    Required Arguments:\n",
    "        - X : ndarray shape (n,d) : feature matrix\n",
    "        - y : ndarray shape (n,)  : positive v. unlabeled component assignments for each instance\n",
    "    Optional Arguments:\n",
    "        - modelFactory : lambda function returning sklearn-style model instance (has fit, fit_predict, predict_proba, ... functions) : default SVC\n",
    "        - KFoldValue : number of folds to use in k-fold cross-validation : default 10\n",
    "        \n",
    "    Returns\n",
    "        - transform_scores : ndarray (n,) : probability that each instance came from labeled positive set\n",
    "        - auc_pu : float : the AUROC of this non-traditional classifier\n",
    "\n",
    "    \"\"\"\n",
    "    transform_scores = np.zeros(y.shape, dtype=float)\n",
    "    # z-score normalization applied globally rather than within each k-fold iteration\n",
    "    X,ss = prepFeatures(X)\n",
    "    kf = StratifiedKFold(n_splits=KFoldValue, shuffle=False)\n",
    "    for train_indices, val_indices in kf.split(X,y):\n",
    "        X_train, y_train = X[train_indices], y[train_indices]\n",
    "        X_val = X[val_indices]\n",
    "        clf = modelFactory()\n",
    "        clf.fit(X_train, y_train)\n",
    "        transform_scores[val_indices] = clf.predict_proba(X_val)[:,1]\n",
    "    auc_pu = roc_auc_score(y, transform_scores)\n",
    "    return transform_scores, auc_pu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test k-fold and oob transform functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from sklearn.datasets import load_wine\n",
    "X,y = load_wine(return_X_y=True)\n",
    "y = y == 1\n",
    "\n",
    "transform_scores, auc_pu = trainOOBClassifier(X,y,modelFactory=lambda: DecisionTreeClassifier())\n",
    "\n",
    "auc_pu\n",
    "\n",
    "transform_scores, auc_pu = trainKFoldClassifier(X,y)\n",
    "\n",
    "auc_pu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def getOptimalTransform(X,y):\n",
    "    \"\"\"\n",
    "    Train the 6 univariate transforms from (Zeiberg 2020) and return the transform scores and auc_pu for the best transform\n",
    "    \n",
    "    Required Arguments:\n",
    "        - X : ndarray shape (n,d) : feature matrix\n",
    "        - y : ndarray shape (n,)  : positive v. unlabeled component assignments for each instance\n",
    "    Returns:\n",
    "        - transform_scores : ndarray (n,) : probability that each instance came from labeled positive set\n",
    "        - auc_pu : float : the AUROC of this non-traditional classifier\n",
    "    \"\"\"\n",
    "    transform_scores, auc_pu = {},{}\n",
    "    models = [(\"nn_1\",lambda: MLPClassifier(hidden_layer_sizes=(1,1)), 100),\n",
    "              (\"nn_5\",lambda: MLPClassifier(hidden_layer_sizes=(1,1)), 100),\n",
    "              (\"nn_25\",lambda: MLPClassifier(hidden_layer_sizes=(1,1)), 100),\n",
    "              (\"rt\",lambda: DecisionTreeClassifier(), 1000),\n",
    "              (\"svm_1\",lambda: SVC(kernel=\"poly\", degree=1, probability=True),10),\n",
    "              (\"svm_2\",lambda: SVC(kernel=\"poly\", degree=1, probability=True),10)]\n",
    "    for model_name, model_factory, n in tqdm(models,total=len(models),desc=\"Training univariate transforms\"):\n",
    "        if \"svm\" in model_name:\n",
    "            scores, auc = trainKFoldClassifier(X,y,modelFactory=model_factory,KFoldValue=n)\n",
    "        else:\n",
    "            scores, auc = trainOOBClassifier(X,y,modelFactory=model_factory, n_estimators=n)\n",
    "        transform_scores[model_name] = scores\n",
    "        auc_pu[model_name] = auc\n",
    "    # Find the best transform\n",
    "    best_auc = .5\n",
    "    best_transform = \"rt\"\n",
    "    for model_name, auc in auc_pu.items():\n",
    "        if auc > best_auc:\n",
    "            best_transform = model_name\n",
    "            best_auc = auc\n",
    "    return transform_scores[best_transform], auc_pu[best_transform]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "getOptimalTransform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
